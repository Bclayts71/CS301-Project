In this branch is the google colab .ipynb file containing the code for semantic segmentation with annealing optimization.

Explanation of code:

The first half of the code regarding the preprocessing of the data is the same procedure as the first semantic segmentation code in Milestone 2. A general overview of this process is that I first get the images and resize them to 256 by 256. Then I divide the pixel values by 255 so that the colors are between 0 and 1. Next, I get the masks and resize those images to 256 by 256. Lastly, I assign the colors an integer value and one hot encode them, so that I can begin the semantic segmentation.

Annealing method explanation:

In machine learning there are often 1 or more hyperparameters that when changed can affect the way the model performs when trained. Some hyperparameters can create an overfit in the model, and some can make the model train too slow. This balancing act is the reason why it is often a time consuming process to choose the correct hyperparameters. The hyperparameter optimization method that I use in my code is called annealing. This method is a combination of two hyperparameter choosing ideas which are random selection and iterative selection. Random selection is when hyperparameters are randomly chosen a certain amount of times and then the hyperparameters with the best model are selected. Iterative selection is when all combinations of a certain list of hyperparameters are run and then of course the hyperparameter with the best model is selected. These methods have problems with being either too inaccurate or too slow. Annealing gets the best of both worlds by trying a lot of different hyperparameter combinations while still being efficient. The method chooses a random state, and then changes one hyperparameter each time, and if the model is better than those hyperparameters are saved.

Annealing code implementation:

For my annealing method I was looking to optimize three hyperparameters which are learning rate, batch_size, and momentum. To begin the annealing process, I first needed an initial state to work off of. The initial state I chose was learning rate = 1, batch size = 8, and momentum = 0. I compiled the model and trained it using unet, and I got my starting model. I saved a couple of variables from this model, which are the accuracy, validation accuracy, and the hyperparameters. I saved the accuracies because these are the statistics I will be comparing to other models in order to see if the hyperparameters performed better or not. Next, I have a while loop that goes 10 iterations which means that I will choose 10 different states of hyperparameters and compare their accuracies. In each iteration I choose a random hyperparameter to change, and then a random value in that hyperparameter list to set it equal to. Following the annealing methodology, I only change that one hyperparameter and the other 2 will be the same as the state before. Once I have the tuple of hyperparameters, I check to see if this state has already been tested. If it has then I will check a new state. If it is a new state then I compile the model using the hyperparameters and train the model. The accuracy and validation accuracy of the model are compared to the best saved accuracy, and if the current model performed better, then I save that model as the best model. Once the while loop finishes, the hyperparameters that are saved in best state, are the ones that gave the best model. Finally, I take those hyperparameters and train a model one more time with the parameters on 100 epochs. This is the model that I use for my final graphs and segmentation images.

Results Obtained:
